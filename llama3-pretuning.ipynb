{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "# %%capture\n# %pip install -U transformers\n# %pip install -U datasets\n# %pip install -U accelerate\n# %pip install -U peft\n%pip install -U trl --quiet\n%pip install -U bitsandbytes --quiet\n%pip install evaluate bert-score rouge_score --quiet",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:24:53.173768Z",
     "iopub.execute_input": "2025-05-14T21:24:53.174228Z",
     "iopub.status.idle": "2025-05-14T21:26:30.875159Z",
     "shell.execute_reply.started": "2025-05-14T21:24:53.174204Z",
     "shell.execute_reply": "2025-05-14T21:26:30.874379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m348.0/348.0 kB\u001B[0m \u001B[31m7.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0mta \u001B[36m0:00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m193.6/193.6 kB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m363.4/363.4 MB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m664.8/664.8 MB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m211.5/211.5 MB\u001B[0m \u001B[31m2.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m56.3/56.3 MB\u001B[0m \u001B[31m27.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m127.9/127.9 MB\u001B[0m \u001B[31m13.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m207.5/207.5 MB\u001B[0m \u001B[31m8.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m0:00:01\u001B[0m00:01\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m21.1/21.1 MB\u001B[0m \u001B[31m71.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25h\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001B[0m\u001B[31m\n\u001B[0mNote: you may need to restart the kernel to use updated packages.\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.1/76.1 MB\u001B[0m \u001B[31m22.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m:00:01\u001B[0m00:01\u001B[0m\n\u001B[?25hNote: you may need to restart the kernel to use updated packages.\n  Preparing metadata (setup.py) ... \u001B[?25l\u001B[?25hdone\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.0/84.0 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m61.1/61.1 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25h  Building wheel for rouge_score (setup.py) ... \u001B[?25l\u001B[?25hdone\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": "%%capture\n%pip install rouge_score",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:26:30.876627Z",
     "iopub.execute_input": "2025-05-14T21:26:30.876935Z",
     "iopub.status.idle": "2025-05-14T21:26:33.897975Z",
     "shell.execute_reply.started": "2025-05-14T21:26:30.876911Z",
     "shell.execute_reply": "2025-05-14T21:26:33.896787Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\nfrom peft import  LoraConfig, get_peft_model\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format, SFTConfig",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:26:33.899002Z",
     "iopub.execute_input": "2025-05-14T21:26:33.899235Z",
     "iopub.status.idle": "2025-05-14T21:27:03.239816Z",
     "shell.execute_reply.started": "2025-05-14T21:26:33.899204Z",
     "shell.execute_reply": "2025-05-14T21:27:03.239178Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "2025-05-14 21:26:46.510902: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1747258006.698979      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1747258006.757511      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "import bitsandbytes as bnb\nfrom sklearn.metrics import f1_score\nimport numpy as np\nimport torch\nfrom typing import Dict\nfrom transformers import EvalPrediction\nfrom evaluate import load\nfrom bert_score import score as bert_score",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:03.241165Z",
     "iopub.execute_input": "2025-05-14T21:27:03.241745Z",
     "iopub.status.idle": "2025-05-14T21:27:03.575274Z",
     "shell.execute_reply.started": "2025-05-14T21:27:03.241698Z",
     "shell.execute_reply": "2025-05-14T21:27:03.574502Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "import torch\nimport gc\nfrom tqdm import tqdm\nfrom evaluate import load\nfrom bert_score import score as bert_score",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:03.576117Z",
     "iopub.execute_input": "2025-05-14T21:27:03.576331Z",
     "iopub.status.idle": "2025-05-14T21:27:03.580597Z",
     "shell.execute_reply.started": "2025-05-14T21:27:03.576314Z",
     "shell.execute_reply": "2025-05-14T21:27:03.579847Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": "from huggingface_hub import notebook_login\n\n# notebook_login()",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:03.581411Z",
     "iopub.execute_input": "2025-05-14T21:27:03.581681Z",
     "iopub.status.idle": "2025-05-14T21:27:03.603198Z",
     "shell.execute_reply.started": "2025-05-14T21:27:03.581657Z",
     "shell.execute_reply": "2025-05-14T21:27:03.602533Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "tokn=\"add_token\"\n",
    "login(token = tokn) # add your token from huggingface"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:03.603847Z",
     "iopub.execute_input": "2025-05-14T21:27:03.604027Z",
     "iopub.status.idle": "2025-05-14T21:27:03.746203Z",
     "shell.execute_reply.started": "2025-05-14T21:27:03.604014Z",
     "shell.execute_reply": "2025-05-14T21:27:03.745616Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "os.environ[\"WANDB_DISABLED\"] = \"true\"",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:03.746938Z",
     "iopub.execute_input": "2025-05-14T21:27:03.747135Z",
     "iopub.status.idle": "2025-05-14T21:27:03.751215Z",
     "shell.execute_reply.started": "2025-05-14T21:27:03.747118Z",
     "shell.execute_reply": "2025-05-14T21:27:03.750644Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": "base_model = \"meta-llama/Llama-3.2-3B\" # To use this form, you must grant access (enter your info, they will grant access within 10 minutes)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:03.751939Z",
     "iopub.execute_input": "2025-05-14T21:27:03.752137Z",
     "iopub.status.idle": "2025-05-14T21:27:03.767030Z",
     "shell.execute_reply.started": "2025-05-14T21:27:03.752114Z",
     "shell.execute_reply": "2025-05-14T21:27:03.766374Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "source": "bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    # llm_int8_enable_fp32_cpu_offload=True\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map={\"\": torch.cuda.current_device()},\n)\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:03.769356Z",
     "iopub.execute_input": "2025-05-14T21:27:03.769544Z",
     "iopub.status.idle": "2025-05-14T21:27:37.318260Z",
     "shell.execute_reply.started": "2025-05-14T21:27:03.769530Z",
     "shell.execute_reply": "2025-05-14T21:27:37.317679Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9fcb9afc7c924604b63ecd52ca24c562"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ea6fc48a86341c4b01128724ae2e972"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e9416fd7475c4dfe8dea9da8985afaed"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a5920b3e22d447468661efc9d8c2cb13"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab090ce53fbc4bbb9ee5b53d850076ce"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5bedee310cb2409f89b496aa21a41c53"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/185 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe831e37cc6f46cabf8e6fb6e7b27a90"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/50.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "57de30cc691242a29d5ad2f0993823b7"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "86e8473a0f7a4e229eef6ef850f43db4"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/301 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a2ce5981cc5940aaabae993f2d6109e7"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": "# Load the same dataset\ndataset = load_dataset(\"deepmind/math_dataset\", \"arithmetic__add_or_sub\", split=\"train[:5000]\")",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:27:37.319024Z",
     "iopub.execute_input": "2025-05-14T21:27:37.319242Z",
     "iopub.status.idle": "2025-05-14T21:29:56.050010Z",
     "shell.execute_reply.started": "2025-05-14T21:27:37.319225Z",
     "shell.execute_reply": "2025-05-14T21:29:56.049440Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/24.8k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "af5ce3d085e945acbb0cbf344cf4690c"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "math_dataset.py:   0%|          | 0.00/8.40k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "da3d0d80264a42cd836fa827de43db8e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": "The repository for deepmind/math_dataset contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/deepmind/math_dataset.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/2.33G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "668bce75469a435195785152e6ff2470"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/1999998 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6e1cf60149b42d6b66d433b18bca79f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/10000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c48a22b85514a39b02f9e90d122955a"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": "import ast\n\ndef clean_byte_like_strings(row):\n    try:\n        row[\"question\"] = ast.literal_eval(row[\"question\"]).strip()\n    except:\n        row[\"question\"] = row[\"question\"].strip()\n\n    try:\n        row[\"answer\"] = ast.literal_eval(row[\"answer\"]).strip()\n    except:\n        row[\"answer\"] = row[\"answer\"].strip()\n        \n    return row\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:56.050755Z",
     "iopub.execute_input": "2025-05-14T21:29:56.050970Z",
     "iopub.status.idle": "2025-05-14T21:29:56.055378Z",
     "shell.execute_reply.started": "2025-05-14T21:29:56.050953Z",
     "shell.execute_reply": "2025-05-14T21:29:56.054686Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "dataset = dataset.map(clean_byte_like_strings)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:56.056017Z",
     "iopub.execute_input": "2025-05-14T21:29:56.056241Z",
     "iopub.status.idle": "2025-05-14T21:29:56.403566Z",
     "shell.execute_reply.started": "2025-05-14T21:29:56.056224Z",
     "shell.execute_reply": "2025-05-14T21:29:56.402856Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e4c814994c1c415081fe6660721489d5"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": "llama_template = \"\"\"<|begin_of_text|>\n{% for message in messages %}\n<|start_header_id|>{{ message['role'] }}<|end_header_id|>\n{{ message['content'] }}<|eot_id|>\n{% endfor %}\"\"\"\n\ndef format_chat_template(row):\n    instruction = \"You are a helpful math tutor.\"\n    messages = [\n        {\"role\": \"system\", \"content\": instruction},\n        {\"role\": \"user\", \"content\": row[\"question\"]},\n        {\"role\": \"assistant\", \"content\": row[\"answer\"].split(\"####\")[-1].strip()}\n    ]\n\n    row[\"text\"] = tokenizer.apply_chat_template(\n        messages,\n        chat_template=llama_template,\n        tokenize=False\n    )\n    return row",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:56.404422Z",
     "iopub.execute_input": "2025-05-14T21:29:56.404702Z",
     "iopub.status.idle": "2025-05-14T21:29:56.409269Z",
     "shell.execute_reply.started": "2025-05-14T21:29:56.404667Z",
     "shell.execute_reply": "2025-05-14T21:29:56.408581Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": "dataset = dataset.map(format_chat_template,num_proc= 4,)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:56.410118Z",
     "iopub.execute_input": "2025-05-14T21:29:56.410337Z",
     "iopub.status.idle": "2025-05-14T21:29:57.293799Z",
     "shell.execute_reply.started": "2025-05-14T21:29:56.410321Z",
     "shell.execute_reply": "2025-05-14T21:29:57.292919Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map (num_proc=4):   0%|          | 0/5000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98bb1de5890a41bf988ef861c9ee66b3"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": "split_dataset = dataset.train_test_split(test_size=0.1, seed=42) # split data to train & test\n\ntrain_dataset = split_dataset[\"train\"]\neval_dataset = split_dataset[\"test\"]",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:57.295046Z",
     "iopub.execute_input": "2025-05-14T21:29:57.295297Z",
     "iopub.status.idle": "2025-05-14T21:29:57.308227Z",
     "shell.execute_reply.started": "2025-05-14T21:29:57.295272Z",
     "shell.execute_reply": "2025-05-14T21:29:57.307555Z"
    }
   },
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": "def find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:57.309161Z",
     "iopub.execute_input": "2025-05-14T21:29:57.309420Z",
     "iopub.status.idle": "2025-05-14T21:29:57.320995Z",
     "shell.execute_reply.started": "2025-05-14T21:29:57.309397Z",
     "shell.execute_reply": "2025-05-14T21:29:57.320339Z"
    }
   },
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": "peft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\", # this means that the model will generate the answer\n    target_modules=modules\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:57.321781Z",
     "iopub.execute_input": "2025-05-14T21:29:57.321997Z",
     "iopub.status.idle": "2025-05-14T21:29:59.013077Z",
     "shell.execute_reply.started": "2025-05-14T21:29:57.321982Z",
     "shell.execute_reply": "2025-05-14T21:29:59.012457Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": "tokenizer.truncation_side = \"left\" # important note: this line is used based on your data template.\ntokenizer.model_max_length = 1024",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:59.013997Z",
     "iopub.execute_input": "2025-05-14T21:29:59.014440Z",
     "iopub.status.idle": "2025-05-14T21:29:59.018170Z",
     "shell.execute_reply.started": "2025-05-14T21:29:59.014406Z",
     "shell.execute_reply": "2025-05-14T21:29:59.017397Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": "def simple_f1(pred, label):\n    pred_tokens = pred.split()\n    label_tokens = label.split()\n    common = set(pred_tokens) & set(label_tokens)\n\n    if len(common) == 0:\n        return 0.0\n\n    precision = len(common) / len(pred_tokens)\n    recall = len(common) / len(label_tokens)\n\n    if precision + recall == 0:\n        return 0.0\n\n    return 2 * (precision * recall) / (precision + recall)\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:59.018979Z",
     "iopub.execute_input": "2025-05-14T21:29:59.019263Z",
     "iopub.status.idle": "2025-05-14T21:29:59.035017Z",
     "shell.execute_reply.started": "2025-05-14T21:29:59.019246Z",
     "shell.execute_reply": "2025-05-14T21:29:59.034260Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": "\nrouge = load(\"rouge\")\n\ndef evaluate_model(model, tokenizer, dataset, max_new_tokens=100, batch_size=16):\n    model.eval()\n    results = []\n    f1_scores = []\n    rouge_preds = []\n    rouge_labels = []\n\n    for i in tqdm(range(0, len(dataset), batch_size)):\n        batch = dataset[i:i+batch_size]\n\n        tokenizer.padding_side = \"left\"\n        inputs = tokenizer(\n            batch[\"text\"],\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=tokenizer.model_max_length,\n        ).to(model.device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=inputs[\"input_ids\"],\n                attention_mask=inputs[\"attention_mask\"],\n                max_new_tokens=max_new_tokens,\n                do_sample=False,\n            )\n\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        decoded_labels = batch[\"answer\"]  # أو العمود الصحيح حسب الداتا\n\n        # Clean up\n        for pred, label in zip(decoded_preds, decoded_labels):\n            pred_clean = pred.strip()\n            label_clean = label.strip()\n\n            # F1\n            f1 = simple_f1(pred_clean, label_clean)\n            f1_scores.append(f1)\n\n            # ROUGE inputs\n            rouge_preds.append(pred_clean)\n            rouge_labels.append(label_clean)\n\n            results.append({\n                \"prediction\": pred_clean,\n                \"reference\": label_clean,\n                \"f1\": f1,\n            })\n\n        del inputs, outputs\n        torch.cuda.empty_cache()\n        gc.collect()\n\n    # F1\n    avg_f1 = sum(f1_scores) / len(f1_scores)\n\n    # ROUGE\n    rouge_result = rouge.compute(predictions=rouge_preds, references=rouge_labels)\n    rougeL = rouge_result[\"rougeL\"]\n\n    # BERTScore\n    P, R, F1 = bert_score(rouge_preds, rouge_labels, lang=\"en\", device=\"cpu\", verbose=False)\n    avg_bert_f1 = F1.mean().item()\n\n    print(f\"\\n Average F1 Score:     {avg_f1:.4f}\")\n    print(f\" ROUGE-L Score:        {rougeL:.4f}\")\n    print(f\" BERTScore (F1):       {avg_bert_f1:.4f}\")\n\n    return {\n        \"f1\": avg_f1,\n        \"rougeL\": rougeL,\n        \"bertscore_f1\": avg_bert_f1,\n        \"results\": results\n    }\n",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:29:59.035960Z",
     "iopub.execute_input": "2025-05-14T21:29:59.036182Z",
     "iopub.status.idle": "2025-05-14T21:30:00.409544Z",
     "shell.execute_reply.started": "2025-05-14T21:29:59.036157Z",
     "shell.execute_reply": "2025-05-14T21:30:00.408814Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c23e876812be481aa037f70398ec1956"
      }
     },
     "metadata": {}
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": "metrics = evaluate_model(model, tokenizer, eval_dataset, max_new_tokens=60)",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:30:00.410280Z",
     "iopub.execute_input": "2025-05-14T21:30:00.410501Z",
     "iopub.status.idle": "2025-05-14T21:42:27.071252Z",
     "shell.execute_reply.started": "2025-05-14T21:30:00.410484Z",
     "shell.execute_reply": "2025-05-14T21:42:27.070560Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "  0%|          | 0/32 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n  3%|▎         | 1/32 [00:12<06:15, 12.11s/it]/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n100%|██████████| 32/32 [06:14<00:00, 11.71s/it]\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0adf9c3aab91467ea88cbb49a76f7bd3"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f589b571bfee4c189eb6ae09b90b0916"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "69b24fe85eb0415497f95fe8de9011fa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0700ba256d1743eaa05fb9bef96d372e"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a970e88f51d4697b9a62c418104588b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a4b9210530e443fa13057e02f0467be"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n Average F1 Score:     0.0744\n ROUGE-L Score:        0.1221\n BERTScore (F1):       0.7855\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "source": "print(metrics['results'][121]['prediction'])\nprint(\"___________\")\nprint(metrics['results'][121]['reference'])",
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-05-14T21:42:27.072383Z",
     "iopub.execute_input": "2025-05-14T21:42:27.072626Z",
     "iopub.status.idle": "2025-05-14T21:42:27.077337Z",
     "shell.execute_reply.started": "2025-05-14T21:42:27.072609Z",
     "shell.execute_reply": "2025-05-14T21:42:27.076572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "system\nYou are a helpful math tutor.\nuser\nAdd -0.2 and -4.936803.\nassistant\n-5.136803\nYou are a helpful math tutor. นาง\nYou are a helpful math tutor. นาง\nYou are a helpful math tutor. นาง\nYou are a helpful math tutor. นาง\nYou are a helpful math tutor. นาง\nYou are a helpful math tutor. นาง\nYou are a helpful math tutor\n___________\n-5.136803\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  }
 ]
}
